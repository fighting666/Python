{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树\n",
    "ID3（基于信息增益）  \n",
    "C4.5（基于信息增益比）  \n",
    "CART（gini指数）\n",
    "\n",
    "entropy：$H(x) = -\\sum_{i=1}^{n}p_i\\log{p_i}$  \n",
    "conditional entropy: $H(X|Y)=\\sum{P(X|Y)}\\log{P(X|Y)}$  \n",
    "information gain : $g(D, A)=H(D)-H(D|A)$  \n",
    "information gain ratio: $g_R(D, A) = \\frac{g(D,A)}{H(A)}$  \n",
    "gini index:$Gini(D)=\\sum_{k=1}^{K}p_k\\log{p_k}=1-\\sum_{k=1}^{K}p_k^2$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 书上题目5.1 示例数据\n",
    "def create_data():\n",
    "    datasets = [\n",
    "        ['青年', '否', '否', '一般', '否'],\n",
    "        ['青年', '否', '否', '好', '否'],\n",
    "        ['青年', '是', '否', '好', '是'],\n",
    "        ['青年', '是', '是', '一般', '是'],\n",
    "        ['青年', '否', '否', '一般', '否'],\n",
    "        ['中年', '否', '否', '一般', '否'],\n",
    "        ['中年', '否', '否', '好', '否'],\n",
    "        ['中年', '是', '是', '好', '是'],\n",
    "        ['中年', '否', '是', '非常好', '是'],\n",
    "        ['中年', '否', '是', '非常好', '是'],\n",
    "        ['老年', '否', '是', '非常好', '是'],\n",
    "        ['老年', '否', '是', '好', '是'],\n",
    "        ['老年', '是', '否', '好', '是'],\n",
    "        ['老年', '是', '否', '非常好', '是'],\n",
    "        ['老年', '否', '否', '一般', '否'],\n",
    "    ]\n",
    "    labels = [u'年龄', u'有工作', u'有自己的房子', u'信贷情况', u'类别']\n",
    "    # 返回数据集和每个维度的名称\n",
    "    return datasets, labels\n",
    "\n",
    "datasets, labels = create_data()\n",
    "train_data = pd.DataFrame(datasets, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息熵\n",
    "def calc_ent(datasets):\n",
    "    data_length = len(datasets)\n",
    "    label_count = {}  # 类别集合\n",
    "    # 计算每个类别的样本个数\n",
    "    for i in range(data_length):\n",
    "        label = datasets[i][-1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "    # 计算样本熵值，对应公式中：H(X)\n",
    "    entropy = -sum([(p / data_length) * math.log(p / data_length, 2)\n",
    "                    for p in label_count.values()])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# 计算经验条件熵,对应公式中：H(X|Y)\n",
    "def cond_ent(datasets, axis=0):\n",
    "    data_length = len(datasets)\n",
    "    feature_sets = {}  # 属性集合\n",
    "    for i in range(data_length):\n",
    "        feature = datasets[i][axis]\n",
    "        if feature not in feature_sets:\n",
    "            feature_sets[feature] = []\n",
    "        feature_sets[feature].append(datasets[i])\n",
    "    #计算以第i个特征进行分类后的熵值，对应公式中：H(X|Y)\n",
    "    cond_entropy = sum(\n",
    "        [(len(p) / data_length) * calc_ent(p) for p in feature_sets.values()])\n",
    "    return cond_entropy\n",
    "\n",
    "\n",
    "# ID3算法：计算信息增益,对应公式中：g(X,Y)=H(X)-H(X|Y)\n",
    "def info_gain(entropy, cond_entropy):\n",
    "    return entropy - cond_entropy\n",
    "\n",
    "\n",
    "# 特征选择\n",
    "def info_gain_train(datasets):\n",
    "    count = len(datasets[0]) - 1  # 特征个数\n",
    "    entropy = calc_ent(datasets)  # 训练集D信息熵\n",
    "    best_feature_ID3 = []\n",
    "    best_feature_C45 = []\n",
    "    for c in range(count):\n",
    "        c_info_gain = info_gain(entropy, cond_ent(datasets,\n",
    "                                                  axis=c))  # ID3算法:计算信息增益\n",
    "        c_feature_gain = c_info_gain / calc_ent(\n",
    "            np.array(datasets)[:, c:c + 1].tolist())  # C4.5算法：计算信息增益比\n",
    "        best_feature_ID3.append((c, c_info_gain))\n",
    "        best_feature_C45.append((c, c_feature_gain))\n",
    "        print('  特征({}) - info_gain - {:.3f}'.format(labels[c], c_info_gain))\n",
    "        print('  特征({}) - c_feature_gain - {:.3f}'.format(\n",
    "            labels[c], c_feature_gain))\n",
    "    # 选取信息增益较大的特征\n",
    "    best_ID3 = max(best_feature_ID3, key=lambda x: x[-1])\n",
    "    best_C45 = max(best_feature_C45, key=lambda x: x[-1])\n",
    "    return ' 特征({})的信息增益最大，选择为根节点特征'.format(\n",
    "        labels[best_ID3[0]]), ' 特征({})的信息增益最大，选择为根节点特征'.format(\n",
    "            labels[best_C45[0]])\n",
    "\n",
    "\n",
    "# 计算D关于各个特征A的值的熵 HA(D)\n",
    "# best_feature_C45 = []\n",
    "# c_feature_gain = c_info_gain / calc_ent(np.array(datasets[:, c]))  # C4.5算法：计算信息增益比\n",
    "# best_feature_C45.append((c, c_feature_gain))\n",
    "# best_C45 = max(best_feature_C45, key=lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  特征(年龄) - info_gain - 0.083\n",
      "  特征(年龄) - c_feature_gain - 0.052\n",
      "  特征(有工作) - info_gain - 0.324\n",
      "  特征(有工作) - c_feature_gain - 0.352\n",
      "  特征(有自己的房子) - info_gain - 0.420\n",
      "  特征(有自己的房子) - c_feature_gain - 0.433\n",
      "  特征(信贷情况) - info_gain - 0.363\n",
      "  特征(信贷情况) - c_feature_gain - 0.232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' 特征(有自己的房子)的信息增益最大，选择为根节点特征', ' 特征(有自己的房子)的信息增益最大，选择为根节点特征')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain_train(np.array(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用ID3算法生成决策树，例5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义节点类 二叉树\n",
    "class Node:\n",
    "    # 初始化属性,在类创建对象时自动执行\n",
    "    def __init__(self, root=True, label=None, feature_name=None, feature=None):\n",
    "        self.root = root  # 根节点\n",
    "        self.label = label  # 类标签\n",
    "        self.feature_name = feature_name  # 特征名称\n",
    "        self.feature = feature  # 特征值\n",
    "        self.tree = {}\n",
    "        self.result = {\n",
    "            'label:': self.label,\n",
    "            'feature': self.feature,\n",
    "            'tree': self.tree\n",
    "        }\n",
    "\n",
    "    # 打印输出结果\n",
    "    def __repr__(self):\n",
    "        return '{}'.format(self.result)\n",
    "\n",
    "    def add_node(self, val, node):\n",
    "        self.tree[val] = node\n",
    "\n",
    "    def predict(self, features):\n",
    "        if self.root is True:\n",
    "            return self.label\n",
    "        return self.tree[features[self.feature]].predict(features)  # ????\n",
    "\n",
    "\n",
    "class DTree:\n",
    "    # 初始化属性,在类创建对象时自动执行\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self._tree = {}\n",
    "\n",
    "    @staticmethod\n",
    "    #     \"\"\"\n",
    "    #     @property：Python内置的@property装饰器就是负责把一个方法变成属性调用\n",
    "    #     @staticmethod：返回函数的静态方法,该方法不强制要求传递参数\n",
    "    #     \"\"\"\n",
    "    # 经验熵H(D)\n",
    "    def calc_ent(datasets):\n",
    "        data_length = len(datasets)\n",
    "        label_count = {}\n",
    "        # 计算每个类别的样本个数\n",
    "        for i in range(data_length):\n",
    "            label = datasets[i][-1]\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        # 计算样本熵值，对应公式中：H(X)\n",
    "        ent = -sum([(p / data_length) * math.log(p / data_length, 2)\n",
    "                    for p in label_count.values()])\n",
    "        return ent\n",
    "\n",
    "    # 经验条件熵,对应公式中：H(X|Y)\n",
    "    def cond_ent(self, datasets, axis=0):\n",
    "        data_length = len(datasets)\n",
    "        feature_sets = {}\n",
    "        for i in range(data_length):\n",
    "            feature = datasets[i][axis]\n",
    "            if feature not in feature_sets:\n",
    "                feature_sets[feature] = []\n",
    "            feature_sets[feature].append(datasets[i])\n",
    "        #计算以第i个特征进行分类后的熵值，对应公式中：H(X|Y)\n",
    "        cond_ent = sum([(len(p) / data_length) * self.calc_ent(p)\n",
    "                        for p in feature_sets.values()])\n",
    "        return cond_ent\n",
    "\n",
    "    # ID3算法：计算信息增益,对应公式中：g(X,Y)=H(X)-H(X|Y)\n",
    "    @staticmethod\n",
    "    def info_gain(ent, cond_ent):\n",
    "        return ent - cond_ent\n",
    "\n",
    "    # 特征选择\n",
    "    def info_gain_train(self, datasets):\n",
    "        count = len(datasets[0]) - 1  # 特征个数\n",
    "        ent = self.calc_ent(datasets)  # 训练集D信息熵\n",
    "        best_feature_ID3 = []\n",
    "        best_feature_C45 = []\n",
    "        for c in range(count):\n",
    "            c_info_gain = self.info_gain(ent, self.cond_ent(\n",
    "                datasets, axis=c))  # ID3算法:计算信息增益\n",
    "            c_feature_gain = c_info_gain / calc_ent(\n",
    "                np.array(datasets)[:, c:c + 1].tolist())  # C4.5算法：计算信息增益比\n",
    "            # print('  特征({}) - info_gain - {:.3f}'.format(features[c], c_info_gain))\n",
    "        best_feature_ID3.append((c, c_info_gain))\n",
    "        best_feature_C45.append((c, c_feature_gain))\n",
    "        # 选取信息增益较大的特征\n",
    "        best_ID3 = max(best_feature_ID3, key=lambda x: x[-1])\n",
    "        best_C45 = max(best_feature_C45, key=lambda x: x[-1])\n",
    "        return best_ID3\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        input:数据集D(DataFrame格式)，特征集A，阈值eta\n",
    "        output:决策树T\n",
    "        \"\"\"\n",
    "        _, y_train, features = train_data.iloc[:, :\n",
    "                                               -1], train_data.iloc[:,\n",
    "                                                                    -1], train_data.columns[:\n",
    "                                                                                            -1]\n",
    "        # 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T\n",
    "        if len(y_train.value_counts()) == 1:\n",
    "            return Node(root=True, label=y_train.iloc[0])\n",
    "\n",
    "        # 2,若A为空，则T为单节点树，将D中实例树最大的类Ck作为该节点的类标记，返回T\n",
    "        if len(features) == 0:\n",
    "            return Node(\n",
    "                root=True,\n",
    "                label=y_train.value_counts().sort_values(\n",
    "                    ascending=False).index[0])\n",
    "\n",
    "        # 3,计算最大信息增益 同5.1,Ag为信息增益最大的特征\n",
    "        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))\n",
    "        max_feature_name = features[max_feature]\n",
    "\n",
    "        # 4,Ag的信息增益小于阈值eta,则置T为单节点树，并将D中是实例数最大的类Ck作为该节点的类标记，返回T\n",
    "        if max_info_gain < self.epsilon:\n",
    "            return Node(\n",
    "                root=True,\n",
    "                label=y_train.value_counts().sort_values(\n",
    "                    ascending=False).index[0])\n",
    "\n",
    "        # 5,构建Ag子集\n",
    "        node_tree = Node(\n",
    "            root=False, feature_name=max_feature_name, feature=max_feature)\n",
    "\n",
    "        feature_list = train_data[max_feature_name].value_counts().index\n",
    "        for f in feature_list:\n",
    "            sub_train_df = train_data.loc[train_data[max_feature_name] ==\n",
    "                                          f].drop([max_feature_name], axis=1)\n",
    "\n",
    "            # 6, 递归生成树\n",
    "            sub_tree = self.train(sub_train_df)\n",
    "            node_tree.add_node(f, sub_tree)\n",
    "\n",
    "        return node_tree\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        self._tree = self.train(train_data)\n",
    "        return self._tree\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self._tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label:': None, 'feature': 3, 'tree': {'好': {'label:': None, 'feature': 2, 'tree': {'否': {'label:': None, 'feature': 1, 'tree': {'是': {'label:': '是', 'feature': None, 'tree': {}}, '否': {'label:': '否', 'feature': None, 'tree': {}}}}, '是': {'label:': '是', 'feature': None, 'tree': {}}}}, '一般': {'label:': None, 'feature': 2, 'tree': {'否': {'label:': '否', 'feature': None, 'tree': {}}, '是': {'label:': '是', 'feature': None, 'tree': {}}}}, '非常好': {'label:': '是', 'feature': None, 'tree': {}}}}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets, labels = create_data()\n",
    "data_df = pd.DataFrame(datasets, columns=labels)\n",
    "dt = DTree()\n",
    "tree = dt.fit(data_df)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'否'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict(['老年', '否', '否', '一般'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用Sklearn包生成决策树 \n",
    "**sklearn.tree.DecisionTreeClassifier**  \n",
    "Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "# 出现错误：\n",
    "# ExecutableNotFound: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on yo\n",
    "# 原因：graphviz本身是一个软件，需要额外下载，并将其bin加入环境变量之中\n",
    "# 在 https://graphviz.gitlab.io/_pages/Download/Download_windows.html 下载graphviz-2.38.zip\n",
    "# 解压到D:\\Program Files (x86)\\graphviz2.38\\release\\bin\n",
    "# # import os\n",
    "# # os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files/graphviz-2.38/release/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取iris示例数据\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 示例数据\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = [\n",
    "        'sepal length', 'sepal width', 'petal length', 'petal width', 'label'\n",
    "    ]\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    return data[:, :2], data[:, -1]\n",
    "\n",
    "\n",
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    ")\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"262pt\" height=\"269pt\"\r\n",
       " viewBox=\"0.00 0.00 262.00 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-265 258,-265 258,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"145,-261 54,-261 54,-193 145,-193 145,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X[2] &lt;= 1.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.48</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 15</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [9, 6]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"91,-149.5 0,-149.5 0,-96.5 91,-96.5 91,-149.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-104.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [6, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.9677,-192.884C76.1453,-181.886 69.6504,-169.617 63.7735,-158.517\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.7827,-156.72 59.0106,-149.52 60.5962,-159.995 66.7827,-156.72\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"51.6584\" y=\"-169.714\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"200,-157 109,-157 109,-89 200,-89 200,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X[1] &lt;= 1.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.444</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 9</text>\r\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 6]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.357,-192.884C121.92,-184.422 126.888,-175.207 131.663,-166.352\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"134.879,-167.762 136.544,-157.299 128.718,-164.44 134.879,-167.762\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"143.714\" y=\"-177.549\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"145,-53 54,-53 54,-0 145,-0 145,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 3</text>\r\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.264,-88.9485C130.206,-80.2579 124.736,-70.8608 119.633,-62.0917\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.534,-60.1189 114.479,-53.2367 116.484,-63.6401 122.534,-60.1189\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"254,-53 163,-53 163,-0 254,-0 254,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"208.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"208.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"middle\" x=\"208.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 6]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M173.387,-88.9485C178.353,-80.2579 183.722,-70.8608 188.733,-62.0917\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.871,-63.6557 193.793,-53.2367 185.793,-60.1826 191.871,-63.6557\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0xb311cf8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_pic = export_graphviz(clf, out_file=\"mytree.pdf\")\n",
    "with open('mytree.pdf') as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, labels = create_data()\n",
    "train_data = pd.DataFrame(datasets, columns=labels)\n",
    "\n",
    "# 数据转化\n",
    "train_data.ix[train_data['年龄'] == '青年', '年龄'] = 1\n",
    "train_data.ix[train_data['年龄'] == '中年', '年龄'] = 2\n",
    "train_data.ix[train_data['年龄'] == '老年', '年龄'] = 3\n",
    "\n",
    "train_data.ix[train_data['信贷情况'] == '非常好', '信贷情况'] = 1\n",
    "train_data.ix[train_data['信贷情况'] == '好', '信贷情况'] = 2\n",
    "train_data.ix[train_data['信贷情况'] == '一般', '信贷情况'] = 3\n",
    "\n",
    "list_label = ['有工作', '有自己的房子', '类别']\n",
    "for i in list_label:\n",
    "    train_data.ix[train_data[i] == '是', i] = 1\n",
    "    train_data.ix[train_data[i] == '否', i] = 2\n",
    "\n",
    "train_data\n",
    "# 生成决策树模型\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(\n",
    "    train_data.iloc[:, :3],\n",
    "    train_data.iloc[:, -1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树可视化\n",
    "tree_pic = export_graphviz(clf, out_file=\"mytree.pdf\")\n",
    "with open('mytree.pdf') as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn决策树算法库\n",
    "\n",
    "（1）分类决策树DecisionTreeClassifier  \n",
    "（2）回归决策树DecisionTreeRegressor  \n",
    "下面对这两个决策树类的一些重要参数进行介绍：\n",
    "\n",
    "**DecisionTreeClassifier参数说明:**  \n",
    "\n",
    "sklearn.tree.DecisionTreeClassifier  \n",
    "( criterion=’gini’,  \n",
    "  splitter=’best’,  \n",
    "  max_depth=None,  \n",
    "  min_samples_split=2,  \n",
    "  min_samples_leaf=1,  \n",
    "  min_weight_fraction_leaf=0.0,  \n",
    "  max_features=None,  \n",
    "  random_state=None,  \n",
    "  max_leaf_nodes=None,  \n",
    "  min_impurity_decrease=0.0,  \n",
    "  min_impurity_split=None,  \n",
    "  class_weight=None,  \n",
    "  presort=False )  \n",
    "- 1. **criterion:** 特征选择标准，gini或者entropy，前者是基尼系数后者是信息熵，两种算法差异不大对准确率无影响，信息墒运算效率低一点,因为它有对数运算.一般说使用默认的基尼系数”gini”就可以了，即CART算法，除非你更喜欢类似ID3, C4.5的最优特征选择方法；  \n",
    "- 2. **splitter:** 特征划分点选择标准，best or random 前者是在所有特征中找最好的切分点 后者随机地在部分划分点中找最佳划分点，默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random” ；  \n",
    "- 3. **max_depth:** 决策树的最大深度，int or None, optional (default=None) 一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间，常用来解决过拟合；    \n",
    "- 4. **min_samples_split:** 子树再划分所需的最小样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值；    \n",
    "- 5. **min_samples_leaf:** 叶子节点的最小样本数，这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝,如果样本量不大，不需要管这个值，大些如10W可是尝试下5；  \n",
    "- 6. **min_weight_fraction_leaf:** 叶子节点最小样本权重和，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝，默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值；    \n",
    "- 7. **max_features:** 划分时考虑的最大特征数，默认None表示考虑所有特征数；'log2'表示划分时考虑log2N个特征；'sqrt'/'auto'表示划分时考虑sqrt(N)个特征；整数表示考虑的特征绝对数；浮点数表示考虑特征数的百分比，特征小于50的时候一般使用None；  \n",
    "- 8. **random_state：** 如果是int，表示随机数字发生器的种子；如果是RandomState，表示随机数字发生器；如果是None，随机数字发生器是np.random使用的RandomState instance；  \n",
    "- 9. **max_leaf_nodes:** 最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，默认是None，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。 \n",
    "- 10. **min_impurity_decrease：** 节点划分的不纯度减小阈值，如果节点划分后不纯度减小量大于该阈值，则对该节点进行划分；\n",
    "- 11. **min_impurity_split:** 节点划分最小不纯度,这个值限制了决策树的增长,如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点；  \n",
    "- 12. **class_weight:** 指定样本各类别的的权重，主要是为了防止偏向于样本数据过多的类别，如果样本类别分布没有明显的偏差，可以使用默认值None；使用'balanced'表示算法会自行计算权重，样本数少的类别权重会高；也可以自行指定权重；\n",
    "- 13. **presort：** 是否预分类数据以加速训练时最好分类的查找。\n",
    "\n",
    "\n",
    "**DecisionTreeRegressor参数说明:**    \n",
    "class sklearn.tree.DecisionTreeRegressor  \n",
    "( criterion=’mse’,  \n",
    "  splitter=’best’,   \n",
    "  max_depth=None,   \n",
    "  min_samples_split=2,   \n",
    "  min_samples_leaf=1,   \n",
    "  min_weight_fraction_leaf=0.0,   \n",
    "  max_features=None,   \n",
    "  random_state=None,   \n",
    "  max_leaf_nodes=None,  \n",
    "  min_impurity_decrease=0.0,   \n",
    "  min_impurity_split=None,   \n",
    "  presort=False )  \n",
    " \n",
    "- 1. criterion：特征选择标准，'mse'/'mae'分别表示基于均方差、均值差绝对值之和来选择特征；\n",
    "其余参数的意义与DecisionTreeClassifier中的一致。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "298px",
    "width": "414px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
