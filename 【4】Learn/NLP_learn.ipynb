{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from snownlp import SnowNLP\n",
    "from textblob import TextBlob\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天|善|智能|是|一个|专注|于|商业|商业智能|智能|BI||数据|数据分析|分析|||数据|数据挖掘|挖掘|和|大数|数据|技术|领域|的|技术|技术社|社区||www|hellobi|com|||内容|从|最初|的|商业|商业智能|智能||BI||领域|也|扩充|充到|了|数据|数据分析|分析|||数据|数据挖掘|挖掘|和|大数|数据|相关|||的|技术|领域|||包括||R|Python|SPSS|Hadoop|Spark|Hive|Kylin|等|||成为|一个|专注|于|数据|领域|的|垂直|社区|||天|善|智能|致力|致力于|构建|一个|基于|数据|领域|的|生态|生态圈|||通过|社区|链接|一切|||与|数据|相关|的|资源|||例如|如数|数据|本身|||人|||数据|方案|供应|供应商|和|企业|||与|大家|一起|共同|共同努力|努力|力推|推动|大数|数据|||商业|商业智能|智能|BI|在|国内|的|普及|和|发展||\n"
     ]
    }
   ],
   "source": [
    "#结巴分词--全模式\n",
    "sentce = '天善智能是一个专注于商业智能BI、数据分析、数据挖掘和大数据技术领域的技术社区 www.hellobi.com 。内容从最初的商业智能 BI 领域也扩充到了数据分析、数据挖掘和大数据相关 的技术领域，包括 R、Python、SPSS、Hadoop、Spark、Hive、Kylin等，成为一个专注于数据领域的垂直社区。天善智能致力于构建一个基于数据领域的生态圈，通过社区链接一切 与数据相关的资源:例如数据本身、人、数据方案供应商和企业，与大家一起共同努力推动大数据、商业智能BI在国内的普及和发展。'\n",
    "wordlist = jieba.cut(sentce, cut_all=True)\n",
    "print(\"|\".join(wordlist))\n",
    "# 全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。这种全模式，会根据字典，将所有出现的字词全部匹配划分，所以会出现重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天善|智能|是|一个|专注|于|商业智能|BI|、|数据分析|、|数据挖掘|和|大|数据|技术|领域|的|技术|社区| |www|.|hellobi|.|com| |。|内容|从|最初|的|商业智能| |BI| |领域|也|扩充|到|了|数据分析|、|数据挖掘|和|大|数据|相关| |的|技术|领域|，|包括| |R|、|Python|、|SPSS|、|Hadoop|、|Spark|、|Hive|、|Kylin|等|，|成为|一个|专注|于|数据|领域|的|垂直|社区|。|天善|智能|致力于|构建|一个|基于|数据|领域|的|生态圈|，|通过|社区|链接|一切| |与|数据|相关|的|资源|:|例如|数据|本身|、|人|、|数据|方案|供应商|和|企业|，|与|大家|一起|共同努力|推动|大|数据|、|商业智能|BI|在|国内|的|普及|和|发展|。\n"
     ]
    }
   ],
   "source": [
    "#结巴分词--精确切分\n",
    "wordlist = jieba.cut(sent)  #cut_all=Flase\n",
    "print(\"|\".join(wordlist))\n",
    "# 精确模式：试图将句子最精确地切开，适合文本分析（类似LTP分词方式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天善|智能|是|一个|专注|于|商业|智能|商业智能|BI|、|数据|分析|数据分析|、|数据|挖掘|数据挖掘|和|大|数据|技术|领域|的|技术|社区| |www|.|hellobi|.|com| |。|内容|从|最初|的|商业|智能|商业智能| |BI| |领域|也|扩充|到|了|数据|分析|数据分析|、|数据|挖掘|数据挖掘|和|大|数据|相关| |的|技术|领域|，|包括| |R|、|Python|、|SPSS|、|Hadoop|、|Spark|、|Hive|、|Kylin|等|，|成为|一个|专注|于|数据|领域|的|垂直|社区|。|天善|智能|致力|致力于|构建|一个|基于|数据|领域|的|生态|生态圈|，|通过|社区|链接|一切| |与|数据|相关|的|资源|:|例如|数据|本身|、|人|、|数据|方案|供应|供应商|和|企业|，|与|大家|一起|共同|努力|共同努力|推动|大|数据|、|商业|智能|商业智能|BI|在|国内|的|普及|和|发展|。\n"
     ]
    }
   ],
   "source": [
    "#结巴分词--搜索引擎模式\n",
    "wordlist = jieba.cut_for_search(sent)\n",
    "print('|'.join(wordlist))\n",
    "# 搜索引擎模式：在精确模式的基础上对长词再次切分，提高召回率，适合用于搜索引擎分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"I am happy today.\"), Sentence(\"I feel sad today.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.5, subjectivity=1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/# 英文情感分析\n",
    "text = \"I am happy today. I feel sad today.\"\n",
    "blob=TextBlob(text)\n",
    "blob.sentences\n",
    "blob.sentences[0].sentiment\n",
    "blob.sentences[1].sentiment\n",
    "\n",
    "# 中文情感分析\n",
    "text = u\"我今天很快乐。我今天很愤怒。\"\n",
    "s=SnowNLP(text)\n",
    "for sentence in s.sentences:\n",
    "    sentence\n",
    "SnowNLP(s.sentences[0]).sentiments\n",
    "SnowNLP(s.sentences[1]).sentiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
